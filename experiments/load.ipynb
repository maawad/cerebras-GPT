{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawad/miniconda3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return pipe, model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../models/Cerebras-GPT-111M/\"\n",
    "pipeline_111m,_ , _ = model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An efficient GPU code balances between \n",
      "   * the GPU and the CPU.\n",
      "\n",
      "The GPU is a GPU with a CPU-specific GPU. The GPU has a\n",
      "CPU-level GPU, and a memory-size of 1.0. This GPU can be\n",
      "used to store the memory of the device. In this case, the\n",
      "GPU is used to compute the hardware.\n",
      "\n",
      "  The CPU is the only GPU that can store a single GPU\n",
      "\t.  This is called the \"GPU\" because it is not a hardware\n",
      "device. It is also called a \"CPU\" and is only used for the other\n",
      " CPU types. A GPU-based GPU will store only a few GPU types\n",
      "(e.g. GPU memory, GPU size, etc.) and only one GPU type\n",
      "is used. For example, aGPU can only store one or two GPU chips\n",
      "in a given GPU (e1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n",
      "17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n"
     ]
    }
   ],
   "source": [
    "text = \"An efficient GPU code balances between \"\n",
    "generated_text = pipeline_111m(text, max_length=256, do_sample=False, no_repeat_ngram_size=2)[0] # type: ignore\n",
    "print(generated_text['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../models/Cerebras-GPT-2.7B/\"\n",
    "pipeline_2p7b,_ , _ = model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An efficient GPU code balances between \n",
      "   *  the number of threads and the amount of memory used.\n",
      "*  @param  numThreads  The number (in the range [1, numGPUs]) of the threads to use.  This parameter must be a power of 2. If the value is not a valid power-of-2, the function will return an error. The default value for this parameter is 1.0. This value can be changed by the user. It is recommended to set this value to a value in the [0, 1] range. */\n",
      "\n",
      "/*! @brief  Returns the maximum number threads that can run in parallel.\n",
      "\n",
      "  If this function is called with the parameter numCPUs, it returns the total number\n",
      "of CPU cores on the system. Otherwise, if this is a GPU-enabled application, this\n",
      "function returns a maximum of numGpuCores. In the latter case,\n",
      "this function returns numCpuThreadCount. For example, to get the max number  of\n",
      "cores on a system with 8 CPU's, call this method with numCPUS = 8 and numGPUS\n",
      "= numCUDACORE. To get numcudaCp\n"
     ]
    }
   ],
   "source": [
    "text = \"An efficient GPU code balances between \"\n",
    "generated_text = pipeline_2p7b(text, max_length=256, do_sample=False, no_repeat_ngram_size=2)[0] # type: ignore\n",
    "print(generated_text['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
